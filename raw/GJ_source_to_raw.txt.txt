from pyspark.context import SparkContext
from awsglue.context import GlueContext
from pyspark.sql import SparkSession
from pyspark.sql.functions import col

# Create a SparkContext and GlueContext
sc = SparkContext()
glueContext = GlueContext(sc)
spark = glueContext.spark_session

# Define the S3 path
source_path = "s3://source-bucket/emp_details/SDF01_20230101.csv"

# Read CSV data into a PySpark DataFrame
source_df = spark.read.csv(source_path, header=True, inferSchema=True)

# Perform type casting transformation on the emp_id column
source_df = source_df.withColumn("emp_id", col("emp_id").cast("int"))



# Write the DataFrame to the RDS table
source_df.write \
    .format("jdbc") \
    .option("url", "jdbc:mysql://your-rds-endpoint:3306/prod_rds_db") \
    .option("dbtable", "tbl_raw_emp_details") \
    .option("user", "your-username") \
    .option("password", "your-password") \
    .mode("overwrite") \
    .save()
