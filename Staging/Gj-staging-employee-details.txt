
import sys
from pyspark.sql.types import StructType, StructField, StringType, IntegerType  # Add these import statements
from awsglue.context import GlueContext
from awsglue.utils import getResolvedOptions
from awsglue.dynamicframe import DynamicFrame
from awsglue.job import Job
from pyspark import SparkContext
from pyspark.sql import SparkSession
from pyspark.sql.functions import to_date, expr, col , concat ,lit, current_date
import boto3

# Initialize SparkContext and GlueContext
args = getResolvedOptions(sys.argv, ['JOB_NAME','keypara'])
sc = SparkContext()
spark = SparkSession.builder.appName("example").getOrCreate()
glueContext = GlueContext(sc)
job = Job(glueContext)
job.init(args['JOB_NAME'], args)

input_path=args['keypara']
#input_path='s3://prod-wayfair-raw-buckett/raw_data/Employee_details/'



# Define the schema for the PySpark DataFrame
schema = StructType([
    StructField("em_date", StringType(), True),
    StructField("employee_id", IntegerType(), True),
    StructField("first_name", StringType(), True),
    StructField("last_name", StringType(), True),
    StructField("batch_id", StringType(), True),
    StructField("location_id", IntegerType(), True),
    StructField("start_date", StringType(), True),
    StructField("end_date", StringType(), True),
    StructField("shift_id", StringType(), True),
    StructField("employee_type", StringType(), True),
    StructField("employee_type_id", StringType(), True),
  	StructField("employee_time_pass", StringType(), True),
	  StructField("employee_time_waste", StringType(), True) 
    ])

# Create a PySpark DataFrame
df = spark.read.format("csv").option("header", True).option("inferSchema", True).load(input_path)

# Redefine schema for date columns:
df = df.withColumn("em_date", to_date(df["em_date"], 'yyyy-MM-dd')) \
    .withColumn("start_date", to_date(df["start_date"], 'yyyy-MM-dd')) \
    .withColumn("end_date", to_date(df["end_date"], 'yyyy-MM-dd')) \
    .where(col("em_date") <= expr("current_date() - interval 1 day"))

df=df.select(col("em_date").alias("date"),col("employee_id"),concat(col("first_name"), lit(' '), col("last_name")).alias("emp_full_name"),col("batch_id"),col("location_id"),col("shift_id"),col("employee_type_id"))



#df.show()
#df.printSchema()

# Writing down to s3 target location:
output_path = "s3://prod-wayfair-staging-bucket-1/staging_data/Employee_details/"
df.coalesce(1).write.option("header", False).format("csv").mode("overwrite").save(output_path)


job_id = args['JOB_ID']
job_name = args['JOB_NAME']
job_run_id = args['JOB_RUN_ID']

print(args)

jdbc_url = "jdbc:sqlserver://wayfair-prod-db.c98yykqo2trj.us-east-1.rds.amazonaws.com:1433;databaseName=prod_wayfair_source"
username = "developer_1"
password = "Wayfair_1234"
dbtable = "dbo.tbl_job_execution_status"

df_jobs_schedule_status= spark.read \
    .format("jdbc") \
    .option("url", jdbc_url) \
    .option("dbtable", dbtable) \
    .option("user", username) \
    .option("password", password) \
    .load().cache()



# making CTE of dataframe
df_jobs_schedule_status.createOrReplaceTempView("vw_job_status")


# Get the current date directly in the query
query = f"""
SELECT
    execution_date,
    job_id,
    job_name,
    CASE WHEN job_id = '{job_id}' AND job_name = '{job_name}' AND execution_date = current_date() THEN '{job_run_id}' ELSE job_run_id END AS job_run_id,
    CASE WHEN job_id = '{job_id}' AND job_name = '{job_name}' AND execution_date = current_date() THEN 1 ELSE is_completed END AS is_completed,
    CASE WHEN job_id = '{job_id}' AND job_name = '{job_name}' AND execution_date = current_date() THEN 'lambda trigger' ELSE job_executed_by END AS job_executed_by
FROM vw_job_status
"""

# Execute the query and store the results
df_jobs_schedule_status = spark.sql(query)
df_jobs_schedule_status.show()
df_jobs_schedule_status.printSchema()




# do not run below part :
df_jobs_schedule_status.write \
    .format("jdbc") \
    .option("url", jdbc_url) \
    .option("dbtable", dbtable) \
    .option("user", username) \
    .option("password", password) \
    .option("driver", "com.microsoft.sqlserver.jdbc.SQLServerDriver") \
    .mode("overwrite").save() 





# Filter for the specific job_id and check is_completed
filtered_df = df_jobs_schedule_status.filter(
    col("job_id").isin(
        "j_c865a434f83a97c27b268a90280946fa68fa9949401b2d0fadf032122d03ad43",
        "j_822355868ae35fa85e926a45f057960f32fb126974e07ea9c7c03f982db58c19",
        "j_8b70ccb9b71110f2012e29032f220746da3aef602eea8d0417306954191e7cd3",
        "j_4112dcf0a94d59f889288219d649f18bb1d28a9ac0fbcb5ce2e0c4411c17852a"
    ))\
    .filter(col("execution_date") == current_date())\
    .filter(col("is_completed") == 1)








# Checking if other dependent jobs are completed. If completed, then we will trigger the foundation job:
if filtered_df.count() == 4:
    print("All staging jobs are completed.")
    print("Invoking lambda function to execute foundation job.")
    
    def invoke_foundation_emp_performance_lambda():
        try:
            lambda_client = boto3.client('lambda')
            function_name = "lm_foundation_employee_performance_metrics"
            lambda_client.invoke(
                FunctionName=function_name,
                InvocationType='RequestResponse'
            )
            
            print("Lambda function for foundation employee performance has been invoked successfully")
        except Exception as e:
            print(f"Error invoking Lambda function: {e}")

    # Invoke the function
    invoke_foundation_emp_performance_lambda()
    
else:
    print("Dependent jobs are not finished yet.")
    

