import sys
from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DateType
from awsglue.context import GlueContext
from awsglue.utils import getResolvedOptions
from awsglue.dynamicframe import DynamicFrame
from awsglue.job import Job
from pyspark import SparkContext
from pyspark.sql import SparkSession
from pyspark.sql.functions import to_date, expr, col

# Initialize SparkContext and GlueContext
args = getResolvedOptions(sys.argv, ['JOB_NAME','keypara'])
spark = SparkSession.builder.appName("example").getOrCreate()
glueContext = GlueContext(SparkContext.getOrCreate())
job = Job(glueContext)
job.init(args['JOB_NAME'], args)

input_path=args['keypara']
# Define the schema for the PySpark DataFrame
schema = StructType([
    StructField("application_id", StringType(), True),
    StructField("application_date", StringType(), True),
    StructField("leave_date", StringType(), True),
    StructField("employee_id", IntegerType(), True),
    StructField("manager_id", StringType(), True),
    StructField("approval_status", StringType(), True),
    StructField("leave_type", StringType(), True),
    StructField("leave_start_timestamp", StringType(), True),
    StructField("leave_end_timestamp", StringType(), True),
    StructField("is_half_day", StringType(), True),
    StructField("updated_timestamp", StringType(), True)
])



# Create a PySpark DataFrame with the defined schema
df = spark.read.format("csv").option("header", True).schema(schema).load(input_path)


#redefininng schema for date columns :
df = df.withColumn("application_date", to_date(df["application_date"], 'yyyy-MM-dd')) \
       .withColumn("leave_date", to_date(df["leave_date"], 'yyyy-MM-dd')) \
    .withColumn("leave_start_timestamp", col("leave_start_timestamp").cast("timestamp")) \
    .withColumn("leave_end_timestamp", col("leave_end_timestamp").cast("timestamp")) \
    .withColumn("updated_timestamp", col("updated_timestamp").cast("timestamp"))



#df.show()
#df.printSchema()
# write into a single CSV file to the target location:
output_path = "s3://prod-wayfair-raw-buckett/raw_data/Employee_leave_details/"
df.coalesce(1).write.option("header", True).format("csv").mode("overwrite").save(output_path)

